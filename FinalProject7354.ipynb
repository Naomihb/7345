{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (50, 100, 100, 3) was passed for an output of shape (None, 10) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-403a789a1f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-403a789a1f2c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;31m#simpleEx(digits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;31m#preTrain(X_train, y_train, X_test, y_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     \u001b[0mconv2DEx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-403a789a1f2c>\u001b[0m in \u001b[0;36mconv2DEx\u001b[1;34m(xTrain, xTest, yTrain, yTest)\u001b[0m\n\u001b[0;32m    143\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                           validation_data=(xTest, yTest))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2655\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2656\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2657\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2658\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    510\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    511\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (50, 100, 100, 3) was passed for an output of shape (None, 10) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "# Resources\n",
    "# https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/\n",
    "# https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "# https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n",
    "# https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a\n",
    "\n",
    "# Standard scientific Python imports\n",
    "import tensorflow.python.keras as k\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.core import Dropout, Flatten, Activation\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def simpleEx(digits):\n",
    "    # The data that we are interested in is made of 8x8 images of digits, let's\n",
    "    # have a look at the first 4 images, stored in the `images` attribute of the\n",
    "    # dataset.  If we were working from image files, we could load them using\n",
    "    # matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "    # images, we know which digit they represent: it is given in the 'target' of\n",
    "    # the dataset.\n",
    "    _, axes = plt.subplots(2, 4)\n",
    "    images_and_labels = list(zip(digits.images, digits.target))\n",
    "    for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title('Training: %i' % label)\n",
    "\n",
    "    # To apply a classifier on this data, we need to flatten the image, to\n",
    "    # turn the data in a (samples, feature) matrix:\n",
    "    n_samples = len(digits.images)\n",
    "    data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "    # Create a classifier: a support vector classifier\n",
    "    # supervised learning models with associated learning algorithms \n",
    "    #that analyze data used for classification and regression analysis. \n",
    "    classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "    # Split data into train and test subsets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "    # We learn the digits on the first half of the digits\n",
    "    t0 = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"Time for simpleEx \", total)\n",
    "\n",
    "    # Now predict the value of the digit on the second half:\n",
    "    predicted = classifier.predict(X_test)\n",
    "\n",
    "    images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "    for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title('Prediction: %i' % prediction)\n",
    "\n",
    "    print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "          % (classifier, metrics.classification_report(y_test, predicted)))\n",
    "    # isp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
    "    # isp.figure_.suptitle(\"Confusion Matrix\")\n",
    "    # rint(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
    "    # lt.show()\n",
    "\n",
    "\n",
    "def reshape(X_train, X_test, y_train, y_test):\n",
    "    img_rows = 100;\n",
    "    img_cols = 100;\n",
    "    # X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    # X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    # input_shape = (1, img_rows, img_cols)\n",
    "    # else\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    # more reshaping\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    y_train = y_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    y_test = y_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "    v# more reshaping\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    #num_category = 10\n",
    "    # convert class vectors to binary class matrices\n",
    "    #y_train = k.utils.to_categorical(y_train, num_category)\n",
    "    #y_test = k.utils.to_categorical(y_test, num_category)\n",
    "    print(X_train.shape)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def conv2DEx(xTrain, xTest, yTrain, yTest):\n",
    "    #xTrain, xTest, yTrain, yTest = reshape(X_train, X_test, y_train, y_test)\n",
    "    ##model building\n",
    "    #is a linear stack of Layers. You can create a Sequential model and \n",
    "    #define all of the layers in the constructor\n",
    "    model = Sequential()\n",
    "    # convolutional layer with rectified linear unit activation\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(100, 100, 3)))\n",
    "    # 32 convolution filters used each of size 3x3\n",
    "    # again\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    # 64 convolution filters used each of size 3x3\n",
    "    # choose the best features via pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # randomly turn neurons on and off to improve convergence\n",
    "    model.add(Dropout(0.25))\n",
    "    # flatten since too many dimensions, we only want a classification output\n",
    "    model.add(Flatten())\n",
    "    # fully connected to get all relevant data\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # one more dropout for convergence' sake :) \n",
    "    model.add(Dropout(0.5))\n",
    "    # output a softmax to squash the matrix into output probabilities\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "    # categorical ce since we have multiple classes (10) \n",
    "    model.compile(loss=k.losses.categorical_crossentropy,\n",
    "                  optimizer=k.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    batch_size = 128\n",
    "    num_epoch = 10\n",
    "    # model training\n",
    "    t0 = time.time()\n",
    "    model_log = model.fit(xTrain, yTrain,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=num_epoch,\n",
    "                          verbose=1,\n",
    "                          validation_data=(xTest, yTest))\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"Time for conv2DEx \", total)\n",
    "\n",
    "    # define, fit and evaluate the base autoencoder\n",
    "\n",
    "\n",
    "\n",
    "# evaluate the autoencoder as a classifier\n",
    "def evaluateAutoencoder(model, xTrain, xTest, yTrain, yTest):\n",
    "    batch_size = 128\n",
    "    num_epoch = 10\n",
    "    t0 = time.time()\n",
    "    model_log = model.fit(xTrain, yTrain,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=num_epoch,\n",
    "                          verbose=1,\n",
    "                          validation_data=(xTest, yTest))\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"Time for evaluateAutoencoder \", total)\n",
    "\n",
    "\n",
    "# add one new layer and re-train only the new layer\n",
    "def add_layer_to_autoencoder(model, xTrain, xTest, yTrain, yTest):\n",
    "    # remember the current output layer\n",
    "    output_layer = model.layers[-1]\n",
    "    # remove the output layer\n",
    "    model.pop()\n",
    "    # mark all remaining layers as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add a new hidden layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # re-add the output layer\n",
    "    model.add(output_layer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def base_autoencoder(xTrain, xTest, yTrain, yTest):\n",
    "    model = Sequential()\n",
    "    # convolutional layer with rectified linear unit activation\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(100, 100, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    # fully connected to get all relevant data\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # one more dropout for convergence' sake :) \n",
    "    model.add(Dropout(0.5))\n",
    "    # output a softmax to squash the matrix into output probabilities\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss=k.losses.categorical_crossentropy,\n",
    "                  optimizer=k.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    batch_size = 128\n",
    "    num_epoch = 10\n",
    "    # model training\n",
    "    t0 = time.time()\n",
    "    model_log = model.fit(xTrain, yTrain,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=num_epoch,\n",
    "                          verbose=1,\n",
    "                          validation_data=(xTest, yTest))\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"Time for baseAutoEncoder \", total)\n",
    "    return model\n",
    "\n",
    "def preTrain(X_train, y_train, X_test, y_test):\n",
    "    xTrain, xTest, yTrain, yTest = reshape(X_train, X_test, y_train, y_test)\n",
    "    ##model building\n",
    "    model = base_autoencoder(xTrain, xTest, yTrain, yTest)\n",
    "    model = add_layer_to_autoencoder(model, xTrain, xTest, yTrain, yTest)\n",
    "    evaluateAutoencoder(model, xTrain, xTest, yTrain, yTest)\n",
    "\n",
    "def getData():\n",
    "    # point to my own data, will show the first one just as a sanity check\n",
    "    IMG_SIZE = 100\n",
    "    trainingData = []\n",
    "    newResizeImg = []\n",
    "    DATADIR = os.getcwd() + \"\\\\OneDrive\\\\Documents\\\\Spring 2020\\\\CleanData\\\\training\\\\\"\n",
    "    for img in os.listdir(DATADIR):  # iterate over each image per dress and shoe\n",
    "        newImg = cv2.imread(os.path.join(DATADIR, img))\n",
    "        # newResizeImg = newImg\n",
    "        newResizeImg = cv2.resize(newImg, (IMG_SIZE, IMG_SIZE))\n",
    "        trainingData.append(newResizeImg)\n",
    "        # plt.imshow(img_array)  # graph it\n",
    "        # plt.show()  # display!\n",
    "        # we just want one for now so break\n",
    "    print(len(trainingData));\n",
    "    testX = trainingData[152:202]\n",
    "    testY = trainingData[101:151]\n",
    "    trainX = trainingData[0:50]\n",
    "    trainY = trainingData[51:100]\n",
    "    testX = np.asarray(testX)\n",
    "    testY = np.asarray(testY)\n",
    "    trainY = np.asarray(trainY)\n",
    "    trainX = np.asarray(trainX)\n",
    "    return testX, testY, trainX, trainY\n",
    "def main():\n",
    "    testX, testY, trainX, trainY = getData()\n",
    "    # The digits dataset\n",
    "    #(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    #digits = datasets.load_digits()\n",
    "    #simpleEx(digits)\n",
    "    #preTrain(X_train, y_train, X_test, y_test)\n",
    "    conv2DEx(testX, testY, trainX, trainY)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
