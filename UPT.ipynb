{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "56\n",
      "56\n",
      "56\n",
      "(661, 570, 3)\n",
      "[INFO] compiling model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 657, 566, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 328, 283, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2 (Batc (None, 328, 283, 32)      1312      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 328, 283, 32)      0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 328, 283, 2)       66        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 328, 283, 2)       0         \n",
      "=================================================================\n",
      "Total params: 3,810\n",
      "Trainable params: 3,154\n",
      "Non-trainable params: 656\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (661, 570, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ff7f57002698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-ff7f57002698>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;31m#print(test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;31m# get the base autoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;31m# evaluate the base model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-ff7f57002698>\u001b[0m in \u001b[0;36mbase_autoencoder\u001b[1;34m(train, test)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mtrain_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mtest_mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'> reconstruction error train=%.3f, test=%.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_mse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_mse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m   1032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2594\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2596\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    338\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (661, 570, 3)"
     ]
    }
   ],
   "source": [
    "#Resources\n",
    "#https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/\n",
    "#https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "#https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py\n",
    "#https://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers.core import Activation\n",
    "from tensorflow.python.keras.layers.core import Flatten\n",
    "from tensorflow.python.keras.layers.core import Dropout\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from os.path import isfile, join\n",
    "from matplotlib import pyplot\n",
    "from os import listdir, getcwd, chdir\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data():\n",
    "    #chdir(getcwd() +\"\\\\PycharmProjects\\\\UnsupervisedPre-Training\\\\training-20200217T024831Z-001\\\\training\")\n",
    "    #onlyfiles = [f for f in listdir(getcwd()) if isfile(join(getcwd(), f))]\n",
    "    #print(onlyfiles)\n",
    "    path = getcwd() +\"\\\\PycharmProjects\\\\UnsupervisedPre-Training\\\\training-20200217T024831Z-001\\\\training\\\\*.*\"\n",
    "    data = []\n",
    "    for file in glob.glob(path):\n",
    "        #print(file)\n",
    "        img = cv2.imread(file)\n",
    "        np.resize(img, (661, 570, 3))\n",
    "        data.append (img)\n",
    "    data0 = np.array(data)\n",
    "    #data = data0.reshape(223,661,70,3)\n",
    "    data1 = data[0:55]\n",
    "    data2 = data[55:111]\n",
    "    data3 = data[111:167]\n",
    "    data4 = data[167:223]\n",
    "    print(len(data1))\n",
    "    print(len(data2))\n",
    "    print(len(data3))\n",
    "    print(len(data4))\n",
    "    #img = load_img(\"FashionNova-Heels.jpg\")  # this is a PIL image\n",
    "    #x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "    #x = x.reshape((1,) + x.shape)\n",
    "    return data1, data2, data3, data4\n",
    "\n",
    "\n",
    "# define, fit and evaluate the base autoencoder\n",
    "def base_autoencoder(train, test):\n",
    "    chanDim = 1\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    #print(train.shape)\n",
    "    print(train[0].shape)\n",
    "    model.add(Conv2D(32, (5, 5), activation=\"relu\", input_shape=(661, 570, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # softmax classifier\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    opt = Adam(lr=1e-4, decay=1e-4/12)\n",
    "    #model = StridedNet.build(width=96, height=96, depth=3, classes=len(lb.classes_), reg=l2(0.0005))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    print(model.summary())\n",
    "    train_mse = model.evaluate(train, train, verbose=0)\n",
    "    test_mse = model.evaluate(test, test, verbose=0)\n",
    "    print('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate the autoencoder as a classifier\n",
    "def evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy):\n",
    "    # remember the current output layer\n",
    "    output_layer = model.layers[-1]\n",
    "    # remove the output layer\n",
    "    model.pop()\n",
    "    # mark all remaining layers as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # fully-connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    # evaluate model\n",
    "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    _, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    # put the model back together\n",
    "    model.pop()\n",
    "    model.add(output_layer)\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    opt = Adam(lr=1e-4, decay=1e-4 / args[\"epochs\"])\n",
    "    #model = StridedNet.build(width=96, height=96, depth=3, classes=len(lb.classes_), reg=l2(0.0005))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "    return train_acc, test_acc\n",
    "\n",
    "\n",
    "# add one new layer and re-train only the new layer\n",
    "def add_layer_to_autoencoder(model, trainX, testX):\n",
    "    # remember the current output layer\n",
    "    output_layer = model.layers[-1]\n",
    "    # remove the output layer\n",
    "    model.pop()\n",
    "    # mark all remaining layers as non-trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add a new hidden layer\n",
    "    model.add(Conv2D(32, (5, 5), activation=\"relu\", input_shape=(661, 570, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization(axis=chanDim))\n",
    "    model.add(Dropout(0.25))\n",
    "    # re-add the output layer\n",
    "    model.add(output_layer)\n",
    "    # fit model\n",
    "    # evaluate reconstruction loss\n",
    "    print(model.summary())\n",
    "    train_mse = model.evaluate(trainX, trainX, verbose=0)\n",
    "    test_mse = model.evaluate(testX, testX, verbose=0)\n",
    "    print('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))\n",
    "\n",
    "def main():\n",
    "    #prepare data\n",
    "    train1, train2, test1, test2 = prepare_data()\n",
    "    #print(test)\n",
    "    # get the base autoencoder\n",
    "    model = base_autoencoder(train1, test1)\n",
    "    # evaluate the base model\n",
    "    scores = dict()\n",
    "    train_acc, test_acc = evaluate_autoencoder_as_classifier(model, train1, train2, test1, test2)\n",
    "    print('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "    scores[len(model.layers)] = (train_acc, test_acc)\n",
    "    # add layers and evaluate the updated model\n",
    "    n_layers = 5\n",
    "    for _ in range(n_layers):\n",
    "        # add layer\n",
    "        add_layer_to_autoencoder(model, trainX, testX)\n",
    "        # evaluate model\n",
    "        train_acc, test_acc = evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy)\n",
    "        print('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n",
    "        # store scores for plotting\n",
    "        scores[len(model.layers)] = (train_acc, test_acc)\n",
    "    # plot number of added layers vs accuracy\n",
    "    keys = list(scores.keys())\n",
    "    pyplot.plot(keys, [scores[k][0] for k in keys], label='train', marker='.')\n",
    "    pyplot.plot(keys, [scores[k][1] for k in keys], label='test', marker='.')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig(os.getcwd() + \"\\\\PycharmProjects\\\\UnsupervisedPre-Training\\\\test.png\")\n",
    "    pyplot.show()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
